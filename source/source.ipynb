{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:49:24.564383Z","iopub.status.busy":"2024-07-02T20:49:24.564024Z","iopub.status.idle":"2024-07-02T20:49:30.623315Z","shell.execute_reply":"2024-07-02T20:49:30.622542Z","shell.execute_reply.started":"2024-07-02T20:49:24.564350Z"},"executionInfo":{"elapsed":22499,"status":"ok","timestamp":1714730033208,"user":{"displayName":"Nicola Bavaro","userId":"09741569290353242080"},"user_tz":-120},"id":"QL8zsU6PVaPa","outputId":"ad2aa420-367e-4d6a-8477-430a1d132f76","trusted":true},"outputs":[],"source":["#utility imports\n","import torch\n","import torchvision\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from torch import nn\n","from torchvision import transforms, datasets\n","from torchvision.datasets import ImageFolder\n","from torch.utils.data import DataLoader\n","from torchvision.transforms import functional as F\n","from PIL import Image\n","import time\n","import random\n","import sys\n","\n","#N.B. kaggle directories are used in the ipynb\n","sys.path.append('/kaggle/input/project/MLDL2024_project1-master')\n","sys.path.append('/kaggle/input/project/MLDL2024_project1-master/datasets')\n","sys.path.append('/kaggle/input/project/MLDL2024_project1-master/models/bisenet')\n","\n","from train import train, val\n","from utils import poly_lr_scheduler, fast_hist, per_class_iou\n","# Model imports\n","from models.deeplabv2 import deeplabv2\n","from models.bisenet import build_bisenet, build_contextpath\n","from build_bisenet import BiSeNet\n","# Dataset imports\n","from cityscapes import CityScapes\n","from gta5 import GTA5"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:49:33.818063Z","iopub.status.busy":"2024-07-02T20:49:33.817295Z","iopub.status.idle":"2024-07-02T20:49:33.821929Z","shell.execute_reply":"2024-07-02T20:49:33.821002Z","shell.execute_reply.started":"2024-07-02T20:49:33.818031Z"},"trusted":true},"outputs":[],"source":["# 19 semantic classes\n","num_classes = 19  "]},{"cell_type":"markdown","metadata":{},"source":["# DEEPLAB v2"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:25:12.018081Z","iopub.status.busy":"2024-07-02T20:25:12.017397Z","iopub.status.idle":"2024-07-02T20:25:12.049275Z","shell.execute_reply":"2024-07-02T20:25:12.048268Z","shell.execute_reply.started":"2024-07-02T20:25:12.018046Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'cuda'"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# Setup device agnostic code\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","device"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T19:23:17.454777Z","iopub.status.busy":"2024-07-02T19:23:17.453782Z","iopub.status.idle":"2024-07-02T19:23:19.858221Z","shell.execute_reply":"2024-07-02T19:23:19.857276Z","shell.execute_reply.started":"2024-07-02T19:23:17.454741Z"},"executionInfo":{"elapsed":6175,"status":"ok","timestamp":1714730143770,"user":{"displayName":"Nicola Bavaro","userId":"09741569290353242080"},"user_tz":-120},"id":"qlSEUO7GZ9Oi","outputId":"1bbb7307-6239-4d26-d4b6-8a6c7c15f6b3","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Deeplab pretraining loading...\n"]}],"source":["pretrained_model_path = '/kaggle/input/pretraineddeeplab/deeplab_resnet_pretrained_imagenet.pth'\n","deeplab_model = deeplabv2.get_deeplab_v2(num_classes=num_classes, pretrain=True, pretrain_model_path=pretrained_model_path)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T19:23:21.248853Z","iopub.status.busy":"2024-07-02T19:23:21.247922Z","iopub.status.idle":"2024-07-02T19:23:21.255472Z","shell.execute_reply":"2024-07-02T19:23:21.254487Z","shell.execute_reply.started":"2024-07-02T19:23:21.248817Z"},"executionInfo":{"elapsed":5501,"status":"ok","timestamp":1714730151586,"user":{"displayName":"Nicola Bavaro","userId":"09741569290353242080"},"user_tz":-120},"id":"hEiOA86sAN4G","outputId":"43490003-0c96-44e7-b87f-6d3f52480f6f","trusted":true},"outputs":[],"source":["# Loss and optimizer\n","loss_fn = nn.CrossEntropyLoss(ignore_index=255).to(device)\n","optimizer = torch.optim.Adam(deeplab_model.parameters(), lr=1e-3)\n","init_lr = 1e-3"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T19:23:24.089900Z","iopub.status.busy":"2024-07-02T19:23:24.089515Z","iopub.status.idle":"2024-07-02T19:23:24.333060Z","shell.execute_reply":"2024-07-02T19:23:24.332065Z","shell.execute_reply.started":"2024-07-02T19:23:24.089873Z"},"executionInfo":{"elapsed":352,"status":"ok","timestamp":1714730154154,"user":{"displayName":"Nicola Bavaro","userId":"09741569290353242080"},"user_tz":-120},"id":"SZ8ob2QVKE4U","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Training dataset size: 1572\n","Validation dataset size: 500\n"]}],"source":["# Training and Validation Sets: Cityscapes\n","root_dir = '/kaggle/input/citiscapes/Cityscapes/Cityspaces'\n","\n","# Def of transformations for images and labels\n","transform_img = transforms.Compose([\n","    transforms.Resize((512, 1024)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean = [0.485, 0.456, 0.406],\n","                          std = [0.229, 0.224, 0.225])\n","])\n","\n","def transform_label(label):\n","    label = F.resize(label, (512, 1024), interpolation=Image.NEAREST)\n","    label = np.array(label, dtype=np.int64)\n","    return label\n","transform_label = transform_label\n","\n","#Datasets\n","train_dataset = CityScapes(root_dir, split='train', transform_img=transform_img, transform_lab=transform_label)\n","val_dataset = CityScapes(root_dir, split='val', transform_img=transform_img, transform_lab=transform_label)\n","\n","batch_size = 2\n","\n","#DataLoaders\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader=DataLoader(val_dataset, batch_size=batch_size, num_workers=2)\n","\n","# sizes\n","train_size = len(train_dataset)\n","val_size = len(val_dataset)\n","print(\"Training dataset size:\", train_size)\n","print(\"Validation dataset size:\", val_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1459615,"status":"error","timestamp":1714731616510,"user":{"displayName":"Nicola Bavaro","userId":"09741569290353242080"},"user_tz":-120},"id":"XoZjnGZYKT1V","outputId":"a5bec820-f070-496e-c3c3-3c8719d4f6c8","trusted":true},"outputs":[],"source":["# Image printing\n","mean = [0.485, 0.456, 0.406]\n","std = [0.229, 0.224, 0.225]\n","\n","def denormalize(img, mean, std):\n","    img = img * std + mean\n","    return np.clip(img, 0, 1)\n","\n","#Print image and respective label\n","def show_image(img, label):\n","    \"\"\" Mostra un'immagine con la sua etichetta. \"\"\"\n","    img = img.numpy().transpose((1, 2, 0))  #(C, H, W) --> (H, W, C)\n","    \n","    #Denormalization for visualization purposes\n","    img = denormalize(img, mean, std)\n","    unique_labels = np.unique(label)\n","\n","    # Image print\n","    plt.figure(figsize=(12, 6))\n","    plt.subplot(1, 2, 1)\n","    plt.imshow(img)\n","    plt.title('Image')\n","\n","    label = train_dataset.decode_target(label)\n","    # Label print\n","    plt.subplot(1, 2, 2)\n","    plt.imshow(label)\n","    plt.title('Label')\n","    plt.show()\n","\n","# Printing 5 images\n","num_images = 5\n","count = 0\n","\n","for images, labels in train_loader:\n","    for i in range(len(images)):\n","        show_image(images[i], labels[i])\n","        count += 1\n","        if count == num_images:\n","            break\n","    if count == num_images:\n","        break"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T19:19:50.469091Z","iopub.status.busy":"2024-07-02T19:19:50.468687Z","iopub.status.idle":"2024-07-02T19:19:50.473750Z","shell.execute_reply":"2024-07-02T19:19:50.472722Z","shell.execute_reply.started":"2024-07-02T19:19:50.469062Z"},"trusted":true},"outputs":[],"source":["start_epoch = 0"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#Upload already trained model\n","'''\n","checkpoint = torch.load('/kaggle/input/project/MLDL2024_project1-master/deeplab_final_checkpoint.pth')\n","deeplab_model.load_state_dict(checkpoint['model_state_dict'])\n","optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","last_lr = checkpoint['lr']\n","start_epoch = checkpoint['epoch'] + 1\n","optimizer = torch.optim.Adam(deeplab_model.parameters(), lr=1e-3)\n","print(\"Start epoch:\", start_epoch)\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["deeplab_model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Training\n","for i in range(start_epoch, 50):\n","    current_lr = poly_lr_scheduler(optimizer, init_lr, iter = i)\n","    print(f'Epoch: {i}')\n","    train(deeplab_model, optimizer, train_loader, loss_fn, device)\n","    print(f\"Current Learning Rate: {current_lr:.6f}\")\n","    torch.save({\n","            'model_state_dict' : deeplab_model.state_dict(),\n","            'optimizer_state_dict' : optimizer.state_dict(),\n","            'epoch' : i,\n","            'lr': current_lr\n","        }, '/kaggle/working/deeplab_checkpoint.pth')"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T19:24:02.714712Z","iopub.status.busy":"2024-07-02T19:24:02.714325Z","iopub.status.idle":"2024-07-02T19:26:11.088066Z","shell.execute_reply":"2024-07-02T19:26:11.086946Z","shell.execute_reply.started":"2024-07-02T19:24:02.714682Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'Final mIoU:': 0.5699565805376476}\n"]}],"source":["# Validation\n","val_accuracy = val(deeplab_model, val_loader, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install -U fvcore"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T19:26:54.624842Z","iopub.status.busy":"2024-07-02T19:26:54.624391Z","iopub.status.idle":"2024-07-02T19:26:55.399463Z","shell.execute_reply":"2024-07-02T19:26:55.398607Z","shell.execute_reply.started":"2024-07-02T19:26:54.624801Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["FLOPs: | module                         | #parameters or shape   | #flops     |\n","|:-------------------------------|:-----------------------|:-----------|\n","| model                          | 43.901M                | 0.375T     |\n","|  conv1                         |  9.408K                |  1.233G    |\n","|   conv1.weight                 |   (64, 3, 7, 7)        |            |\n","|  bn1                           |  0.128K                |  16.777M   |\n","|   bn1.weight                   |   (64,)                |            |\n","|   bn1.bias                     |   (64,)                |            |\n","|  layer1                        |  0.216M                |  7.155G    |\n","|   layer1.0                     |   75.008K              |   2.487G   |\n","|    layer1.0.conv1              |    4.096K              |    0.136G  |\n","|    layer1.0.bn1                |    0.128K              |    4.244M  |\n","|    layer1.0.conv2              |    36.864K             |    1.222G  |\n","|    layer1.0.bn2                |    0.128K              |    4.244M  |\n","|    layer1.0.conv3              |    16.384K             |    0.543G  |\n","|    layer1.0.bn3                |    0.512K              |    16.974M |\n","|    layer1.0.downsample         |    16.896K             |    0.56G   |\n","|   layer1.1                     |   70.4K                |   2.334G   |\n","|    layer1.1.conv1              |    16.384K             |    0.543G  |\n","|    layer1.1.bn1                |    0.128K              |    4.244M  |\n","|    layer1.1.conv2              |    36.864K             |    1.222G  |\n","|    layer1.1.bn2                |    0.128K              |    4.244M  |\n","|    layer1.1.conv3              |    16.384K             |    0.543G  |\n","|    layer1.1.bn3                |    0.512K              |    16.974M |\n","|   layer1.2                     |   70.4K                |   2.334G   |\n","|    layer1.2.conv1              |    16.384K             |    0.543G  |\n","|    layer1.2.bn1                |    0.128K              |    4.244M  |\n","|    layer1.2.conv2              |    36.864K             |    1.222G  |\n","|    layer1.2.bn2                |    0.128K              |    4.244M  |\n","|    layer1.2.conv3              |    16.384K             |    0.543G  |\n","|    layer1.2.bn3                |    0.512K              |    16.974M |\n","|  layer2                        |  1.22M                 |  10.226G   |\n","|   layer2.0                     |   0.379M               |   3.181G   |\n","|    layer2.0.conv1              |    32.768K             |    0.275G  |\n","|    layer2.0.bn1                |    0.256K              |    2.147M  |\n","|    layer2.0.conv2              |    0.147M              |    1.236G  |\n","|    layer2.0.bn2                |    0.256K              |    2.147M  |\n","|    layer2.0.conv3              |    65.536K             |    0.55G   |\n","|    layer2.0.bn3                |    1.024K              |    8.586M  |\n","|    layer2.0.downsample         |    0.132M              |    1.108G  |\n","|   layer2.1                     |   0.28M                |   2.348G   |\n","|    layer2.1.conv1              |    65.536K             |    0.55G   |\n","|    layer2.1.bn1                |    0.256K              |    2.147M  |\n","|    layer2.1.conv2              |    0.147M              |    1.236G  |\n","|    layer2.1.bn2                |    0.256K              |    2.147M  |\n","|    layer2.1.conv3              |    65.536K             |    0.55G   |\n","|    layer2.1.bn3                |    1.024K              |    8.586M  |\n","|   layer2.2                     |   0.28M                |   2.348G   |\n","|    layer2.2.conv1              |    65.536K             |    0.55G   |\n","|    layer2.2.bn1                |    0.256K              |    2.147M  |\n","|    layer2.2.conv2              |    0.147M              |    1.236G  |\n","|    layer2.2.bn2                |    0.256K              |    2.147M  |\n","|    layer2.2.conv3              |    65.536K             |    0.55G   |\n","|    layer2.2.bn3                |    1.024K              |    8.586M  |\n","|   layer2.3                     |   0.28M                |   2.348G   |\n","|    layer2.3.conv1              |    65.536K             |    0.55G   |\n","|    layer2.3.bn1                |    0.256K              |    2.147M  |\n","|    layer2.3.conv2              |    0.147M              |    1.236G  |\n","|    layer2.3.bn2                |    0.256K              |    2.147M  |\n","|    layer2.3.conv3              |    65.536K             |    0.55G   |\n","|    layer2.3.bn3                |    1.024K              |    8.586M  |\n","|  layer3                        |  26.09M                |  0.219T    |\n","|   layer3.0                     |   1.512M               |   12.682G  |\n","|    layer3.0.conv1              |    0.131M              |    1.099G  |\n","|    layer3.0.bn1                |    0.512K              |    4.293M  |\n","|    layer3.0.conv2              |    0.59M               |    4.946G  |\n","|    layer3.0.bn2                |    0.512K              |    4.293M  |\n","|    layer3.0.conv3              |    0.262M              |    2.198G  |\n","|    layer3.0.bn3                |    2.048K              |    17.172M |\n","|    layer3.0.downsample         |    0.526M              |    4.413G  |\n","|   layer3.1                     |   1.117M               |   9.368G   |\n","|    layer3.1.conv1              |    0.262M              |    2.198G  |\n","|    layer3.1.bn1                |    0.512K              |    4.293M  |\n","|    layer3.1.conv2              |    0.59M               |    4.946G  |\n","|    layer3.1.bn2                |    0.512K              |    4.293M  |\n","|    layer3.1.conv3              |    0.262M              |    2.198G  |\n","|    layer3.1.bn3                |    2.048K              |    17.172M |\n","|   layer3.2                     |   1.117M               |   9.368G   |\n","|    layer3.2.conv1              |    0.262M              |    2.198G  |\n","|    layer3.2.bn1                |    0.512K              |    4.293M  |\n","|    layer3.2.conv2              |    0.59M               |    4.946G  |\n","|    layer3.2.bn2                |    0.512K              |    4.293M  |\n","|    layer3.2.conv3              |    0.262M              |    2.198G  |\n","|    layer3.2.bn3                |    2.048K              |    17.172M |\n","|   layer3.3                     |   1.117M               |   9.368G   |\n","|    layer3.3.conv1              |    0.262M              |    2.198G  |\n","|    layer3.3.bn1                |    0.512K              |    4.293M  |\n","|    layer3.3.conv2              |    0.59M               |    4.946G  |\n","|    layer3.3.bn2                |    0.512K              |    4.293M  |\n","|    layer3.3.conv3              |    0.262M              |    2.198G  |\n","|    layer3.3.bn3                |    2.048K              |    17.172M |\n","|   layer3.4                     |   1.117M               |   9.368G   |\n","|    layer3.4.conv1              |    0.262M              |    2.198G  |\n","|    layer3.4.bn1                |    0.512K              |    4.293M  |\n","|    layer3.4.conv2              |    0.59M               |    4.946G  |\n","|    layer3.4.bn2                |    0.512K              |    4.293M  |\n","|    layer3.4.conv3              |    0.262M              |    2.198G  |\n","|    layer3.4.bn3                |    2.048K              |    17.172M |\n","|   layer3.5                     |   1.117M               |   9.368G   |\n","|    layer3.5.conv1              |    0.262M              |    2.198G  |\n","|    layer3.5.bn1                |    0.512K              |    4.293M  |\n","|    layer3.5.conv2              |    0.59M               |    4.946G  |\n","|    layer3.5.bn2                |    0.512K              |    4.293M  |\n","|    layer3.5.conv3              |    0.262M              |    2.198G  |\n","|    layer3.5.bn3                |    2.048K              |    17.172M |\n","|   layer3.6                     |   1.117M               |   9.368G   |\n","|    layer3.6.conv1              |    0.262M              |    2.198G  |\n","|    layer3.6.bn1                |    0.512K              |    4.293M  |\n","|    layer3.6.conv2              |    0.59M               |    4.946G  |\n","|    layer3.6.bn2                |    0.512K              |    4.293M  |\n","|    layer3.6.conv3              |    0.262M              |    2.198G  |\n","|    layer3.6.bn3                |    2.048K              |    17.172M |\n","|   layer3.7                     |   1.117M               |   9.368G   |\n","|    layer3.7.conv1              |    0.262M              |    2.198G  |\n","|    layer3.7.bn1                |    0.512K              |    4.293M  |\n","|    layer3.7.conv2              |    0.59M               |    4.946G  |\n","|    layer3.7.bn2                |    0.512K              |    4.293M  |\n","|    layer3.7.conv3              |    0.262M              |    2.198G  |\n","|    layer3.7.bn3                |    2.048K              |    17.172M |\n","|   layer3.8                     |   1.117M               |   9.368G   |\n","|    layer3.8.conv1              |    0.262M              |    2.198G  |\n","|    layer3.8.bn1                |    0.512K              |    4.293M  |\n","|    layer3.8.conv2              |    0.59M               |    4.946G  |\n","|    layer3.8.bn2                |    0.512K              |    4.293M  |\n","|    layer3.8.conv3              |    0.262M              |    2.198G  |\n","|    layer3.8.bn3                |    2.048K              |    17.172M |\n","|   layer3.9                     |   1.117M               |   9.368G   |\n","|    layer3.9.conv1              |    0.262M              |    2.198G  |\n","|    layer3.9.bn1                |    0.512K              |    4.293M  |\n","|    layer3.9.conv2              |    0.59M               |    4.946G  |\n","|    layer3.9.bn2                |    0.512K              |    4.293M  |\n","|    layer3.9.conv3              |    0.262M              |    2.198G  |\n","|    layer3.9.bn3                |    2.048K              |    17.172M |\n","|   layer3.10                    |   1.117M               |   9.368G   |\n","|    layer3.10.conv1             |    0.262M              |    2.198G  |\n","|    layer3.10.bn1               |    0.512K              |    4.293M  |\n","|    layer3.10.conv2             |    0.59M               |    4.946G  |\n","|    layer3.10.bn2               |    0.512K              |    4.293M  |\n","|    layer3.10.conv3             |    0.262M              |    2.198G  |\n","|    layer3.10.bn3               |    2.048K              |    17.172M |\n","|   layer3.11                    |   1.117M               |   9.368G   |\n","|    layer3.11.conv1             |    0.262M              |    2.198G  |\n","|    layer3.11.bn1               |    0.512K              |    4.293M  |\n","|    layer3.11.conv2             |    0.59M               |    4.946G  |\n","|    layer3.11.bn2               |    0.512K              |    4.293M  |\n","|    layer3.11.conv3             |    0.262M              |    2.198G  |\n","|    layer3.11.bn3               |    2.048K              |    17.172M |\n","|   layer3.12                    |   1.117M               |   9.368G   |\n","|    layer3.12.conv1             |    0.262M              |    2.198G  |\n","|    layer3.12.bn1               |    0.512K              |    4.293M  |\n","|    layer3.12.conv2             |    0.59M               |    4.946G  |\n","|    layer3.12.bn2               |    0.512K              |    4.293M  |\n","|    layer3.12.conv3             |    0.262M              |    2.198G  |\n","|    layer3.12.bn3               |    2.048K              |    17.172M |\n","|   layer3.13                    |   1.117M               |   9.368G   |\n","|    layer3.13.conv1             |    0.262M              |    2.198G  |\n","|    layer3.13.bn1               |    0.512K              |    4.293M  |\n","|    layer3.13.conv2             |    0.59M               |    4.946G  |\n","|    layer3.13.bn2               |    0.512K              |    4.293M  |\n","|    layer3.13.conv3             |    0.262M              |    2.198G  |\n","|    layer3.13.bn3               |    2.048K              |    17.172M |\n","|   layer3.14                    |   1.117M               |   9.368G   |\n","|    layer3.14.conv1             |    0.262M              |    2.198G  |\n","|    layer3.14.bn1               |    0.512K              |    4.293M  |\n","|    layer3.14.conv2             |    0.59M               |    4.946G  |\n","|    layer3.14.bn2               |    0.512K              |    4.293M  |\n","|    layer3.14.conv3             |    0.262M              |    2.198G  |\n","|    layer3.14.bn3               |    2.048K              |    17.172M |\n","|   layer3.15                    |   1.117M               |   9.368G   |\n","|    layer3.15.conv1             |    0.262M              |    2.198G  |\n","|    layer3.15.bn1               |    0.512K              |    4.293M  |\n","|    layer3.15.conv2             |    0.59M               |    4.946G  |\n","|    layer3.15.bn2               |    0.512K              |    4.293M  |\n","|    layer3.15.conv3             |    0.262M              |    2.198G  |\n","|    layer3.15.bn3               |    2.048K              |    17.172M |\n","|   layer3.16                    |   1.117M               |   9.368G   |\n","|    layer3.16.conv1             |    0.262M              |    2.198G  |\n","|    layer3.16.bn1               |    0.512K              |    4.293M  |\n","|    layer3.16.conv2             |    0.59M               |    4.946G  |\n","|    layer3.16.bn2               |    0.512K              |    4.293M  |\n","|    layer3.16.conv3             |    0.262M              |    2.198G  |\n","|    layer3.16.bn3               |    2.048K              |    17.172M |\n","|   layer3.17                    |   1.117M               |   9.368G   |\n","|    layer3.17.conv1             |    0.262M              |    2.198G  |\n","|    layer3.17.bn1               |    0.512K              |    4.293M  |\n","|    layer3.17.conv2             |    0.59M               |    4.946G  |\n","|    layer3.17.bn2               |    0.512K              |    4.293M  |\n","|    layer3.17.conv3             |    0.262M              |    2.198G  |\n","|    layer3.17.bn3               |    2.048K              |    17.172M |\n","|   layer3.18                    |   1.117M               |   9.368G   |\n","|    layer3.18.conv1             |    0.262M              |    2.198G  |\n","|    layer3.18.bn1               |    0.512K              |    4.293M  |\n","|    layer3.18.conv2             |    0.59M               |    4.946G  |\n","|    layer3.18.bn2               |    0.512K              |    4.293M  |\n","|    layer3.18.conv3             |    0.262M              |    2.198G  |\n","|    layer3.18.bn3               |    2.048K              |    17.172M |\n","|   layer3.19                    |   1.117M               |   9.368G   |\n","|    layer3.19.conv1             |    0.262M              |    2.198G  |\n","|    layer3.19.bn1               |    0.512K              |    4.293M  |\n","|    layer3.19.conv2             |    0.59M               |    4.946G  |\n","|    layer3.19.bn2               |    0.512K              |    4.293M  |\n","|    layer3.19.conv3             |    0.262M              |    2.198G  |\n","|    layer3.19.bn3               |    2.048K              |    17.172M |\n","|   layer3.20                    |   1.117M               |   9.368G   |\n","|    layer3.20.conv1             |    0.262M              |    2.198G  |\n","|    layer3.20.bn1               |    0.512K              |    4.293M  |\n","|    layer3.20.conv2             |    0.59M               |    4.946G  |\n","|    layer3.20.bn2               |    0.512K              |    4.293M  |\n","|    layer3.20.conv3             |    0.262M              |    2.198G  |\n","|    layer3.20.bn3               |    2.048K              |    17.172M |\n","|   layer3.21                    |   1.117M               |   9.368G   |\n","|    layer3.21.conv1             |    0.262M              |    2.198G  |\n","|    layer3.21.bn1               |    0.512K              |    4.293M  |\n","|    layer3.21.conv2             |    0.59M               |    4.946G  |\n","|    layer3.21.bn2               |    0.512K              |    4.293M  |\n","|    layer3.21.conv3             |    0.262M              |    2.198G  |\n","|    layer3.21.bn3               |    2.048K              |    17.172M |\n","|   layer3.22                    |   1.117M               |   9.368G   |\n","|    layer3.22.conv1             |    0.262M              |    2.198G  |\n","|    layer3.22.bn1               |    0.512K              |    4.293M  |\n","|    layer3.22.conv2             |    0.59M               |    4.946G  |\n","|    layer3.22.bn2               |    0.512K              |    4.293M  |\n","|    layer3.22.conv3             |    0.262M              |    2.198G  |\n","|    layer3.22.bn3               |    2.048K              |    17.172M |\n","|  layer4                        |  14.965M               |  0.125T    |\n","|   layer4.0                     |   6.04M                |   50.642G  |\n","|    layer4.0.conv1              |    0.524M              |    4.396G  |\n","|    layer4.0.bn1                |    1.024K              |    8.586M  |\n","|    layer4.0.conv2              |    2.359M              |    19.783G |\n","|    layer4.0.bn2                |    1.024K              |    8.586M  |\n","|    layer4.0.conv3              |    1.049M              |    8.792G  |\n","|    layer4.0.bn3                |    4.096K              |    34.345M |\n","|    layer4.0.downsample         |    2.101M              |    17.619G |\n","|   layer4.1                     |   4.463M               |   37.419G  |\n","|    layer4.1.conv1              |    1.049M              |    8.792G  |\n","|    layer4.1.bn1                |    1.024K              |    8.586M  |\n","|    layer4.1.conv2              |    2.359M              |    19.783G |\n","|    layer4.1.bn2                |    1.024K              |    8.586M  |\n","|    layer4.1.conv3              |    1.049M              |    8.792G  |\n","|    layer4.1.bn3                |    4.096K              |    34.345M |\n","|   layer4.2                     |   4.463M               |   37.419G  |\n","|    layer4.2.conv1              |    1.049M              |    8.792G  |\n","|    layer4.2.bn1                |    1.024K              |    8.586M  |\n","|    layer4.2.conv2              |    2.359M              |    19.783G |\n","|    layer4.2.bn2                |    1.024K              |    8.586M  |\n","|    layer4.2.conv3              |    1.049M              |    8.792G  |\n","|    layer4.2.bn3                |    4.096K              |    34.345M |\n","|  layer6.conv2d_list            |  1.401M                |  11.746G   |\n","|   layer6.conv2d_list.0         |   0.35M                |   2.936G   |\n","|    layer6.conv2d_list.0.weight |    (19, 2048, 3, 3)    |            |\n","|    layer6.conv2d_list.0.bias   |    (19,)               |            |\n","|   layer6.conv2d_list.1         |   0.35M                |   2.936G   |\n","|    layer6.conv2d_list.1.weight |    (19, 2048, 3, 3)    |            |\n","|    layer6.conv2d_list.1.bias   |    (19,)               |            |\n","|   layer6.conv2d_list.2         |   0.35M                |   2.936G   |\n","|    layer6.conv2d_list.2.weight |    (19, 2048, 3, 3)    |            |\n","|    layer6.conv2d_list.2.bias   |    (19,)               |            |\n","|   layer6.conv2d_list.3         |   0.35M                |   2.936G   |\n","|    layer6.conv2d_list.3.weight |    (19, 2048, 3, 3)    |            |\n","|    layer6.conv2d_list.3.bias   |    (19,)               |            |\n"]}],"source":["#FLOPs\n","from fvcore.nn import FlopCountAnalysis, flop_count_table\n","\n","height, width = 512, 1024  # Sample input size\n","sample_input = torch.zeros((1, 3, height, width)).to(device)\n","flops = FlopCountAnalysis(deeplab_model, sample_input)\n","print(f\"FLOPs: {flop_count_table(flops)}\")"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T19:27:08.028757Z","iopub.status.busy":"2024-07-02T19:27:08.028383Z","iopub.status.idle":"2024-07-02T19:31:14.228448Z","shell.execute_reply":"2024-07-02T19:31:14.227478Z","shell.execute_reply.started":"2024-07-02T19:27:08.028725Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Mean Latency 246.16708111763\n"," STD Latency 8.362497980959024\n"," Mean FPS 4.114738322070233\n"," STD FPS : 1.7446555297303612\n"]}],"source":["#Latency\n","height = 512\n","width = 1024\n","image = torch.rand(1, 3, height, width)\n","iterations = 1000\n","latency = []\n","FPS = []\n","\n","for i in range(iterations):\n","    start = time.time()\n","    output = deeplab_model(image.to(device))\n","    end = time.time()\n","    l = end - start\n","    latency.append(l)\n","    fps = 1/l\n","    FPS.append(fps)\n","\n","meanLatency = np.mean(latency)*1000\n","stdLatency = np.std(latency)*1000\n","meanFPS = np.mean(FPS)\n","stdFPS = np.std(FPS)\n","\n","print(f'Mean Latency {meanLatency}\\n STD Latency {stdLatency}\\n Mean FPS {meanFPS}\\n STD FPS : {stdFPS}')"]},{"cell_type":"markdown","metadata":{},"source":["# BISENET"]},{"cell_type":"markdown","metadata":{},"source":["* Train & Validation: Cityscapes"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T19:58:51.645493Z","iopub.status.busy":"2024-07-02T19:58:51.644802Z","iopub.status.idle":"2024-07-02T19:58:51.703310Z","shell.execute_reply":"2024-07-02T19:58:51.702287Z","shell.execute_reply.started":"2024-07-02T19:58:51.645462Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'cuda'"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# Setup device agnostic code\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","device"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T19:32:43.037527Z","iopub.status.busy":"2024-07-02T19:32:43.036870Z","iopub.status.idle":"2024-07-02T19:32:46.045289Z","shell.execute_reply":"2024-07-02T19:32:46.044531Z","shell.execute_reply.started":"2024-07-02T19:32:43.037494Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|██████████| 44.7M/44.7M [00:00<00:00, 133MB/s] \n","Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n","100%|██████████| 171M/171M [00:01<00:00, 138MB/s]  \n"]}],"source":["bisenet_model = BiSeNet(num_classes=num_classes, context_path='resnet18')"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T19:32:47.625374Z","iopub.status.busy":"2024-07-02T19:32:47.624565Z","iopub.status.idle":"2024-07-02T19:32:47.630862Z","shell.execute_reply":"2024-07-02T19:32:47.629914Z","shell.execute_reply.started":"2024-07-02T19:32:47.625343Z"},"trusted":true},"outputs":[],"source":["# Loss and optimizer\n","loss_fn = nn.CrossEntropyLoss(ignore_index=255).to(device)\n","optimizer = torch.optim.Adam(bisenet_model.parameters(), lr=1e-3)\n","init_lr = 1e-3"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T19:32:54.184425Z","iopub.status.busy":"2024-07-02T19:32:54.183736Z","iopub.status.idle":"2024-07-02T19:32:54.218662Z","shell.execute_reply":"2024-07-02T19:32:54.217830Z","shell.execute_reply.started":"2024-07-02T19:32:54.184394Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Training dataset size: 1572\n","Validation dataset size: 500\n"]}],"source":["# Training and Validation Sets: Cityscapes\n","root_dir = '/kaggle/input/citiscapes/Cityscapes/Cityspaces'\n","\n","# Def of transformations for images and labels\n","transform_img = transforms.Compose([\n","    transforms.Resize((512, 1024)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean = [0.485, 0.456, 0.406],\n","                          std = [0.229, 0.224, 0.225])\n","])\n","\n","def transform_label(label):\n","    label = F.resize(label, (512, 1024), interpolation=Image.NEAREST)\n","    label = np.array(label, dtype=np.int64)\n","    return label\n","transform_label = transform_label\n","\n","# Datasets\n","train_dataset = CityScapes(root_dir, split='train', transform_img=transform_img, transform_lab=transform_label)\n","val_dataset=CityScapes(root_dir, split='val', transform_img=transform_img, transform_lab=transform_label)\n","\n","batch_size = 4\n","\n","# DataLoaders\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader=DataLoader(val_dataset, batch_size=batch_size, num_workers=2)\n","\n","# sizes\n","train_size = len(train_dataset)\n","val_size = len(val_dataset)\n","print(\"Training dataset size:\", train_size)\n","print(\"Validation dataset size:\", val_size)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["start_epoch = 0"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Upload the already trained model\n","'''\n","checkpoint = torch.load('/kaggle/input/project/MLDL2024_project1-master/bisenet_final_checkpoint.pth')\n","bisenet_model.load_state_dict(checkpoint['model_state_dict'])\n","optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","last_lr = checkpoint['lr']\n","start_epoch = checkpoint['epoch'] + 1\n","optimizer = torch.optim.Adam(bisenet_model.parameters(), lr=1e-3)\n","print(start_epoch)\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["bisenet_model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Training\n","for i in range(start_epoch,50):\n","    current_lr = poly_lr_scheduler(optimizer, init_lr, iter = i)\n","    print(f'Epoch: {i}')\n","    train(bisenet_model, optimizer, train_loader, loss_fn,device)\n","    print(f\"Current Learning Rate: {current_lr:.6f}\")\n","    torch.save({\n","          'model_state_dict' : bisenet_model.state_dict(),\n","          'optimizer_state_dict' : optimizer.state_dict(),\n","          'epoch' : i,\n","          'lr': current_lr\n","      }, '/kaggle/working/bisenet_checkpoint.pth')"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T19:33:50.796313Z","iopub.status.busy":"2024-07-02T19:33:50.795434Z","iopub.status.idle":"2024-07-02T19:34:27.560467Z","shell.execute_reply":"2024-07-02T19:34:27.559337Z","shell.execute_reply.started":"2024-07-02T19:33:50.796271Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'Final mIoU:': 0.5248672576178455}\n"]}],"source":["# Validation\n","val_accuracy = val(bisenet_model, val_loader, device)"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T19:34:45.276954Z","iopub.status.busy":"2024-07-02T19:34:45.276537Z","iopub.status.idle":"2024-07-02T19:34:45.492157Z","shell.execute_reply":"2024-07-02T19:34:45.491269Z","shell.execute_reply.started":"2024-07-02T19:34:45.276918Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["FLOPs: | module                                      | #parameters or shape   | #flops     |\n","|:--------------------------------------------|:-----------------------|:-----------|\n","| model                                       | 12.582M                | 25.78G     |\n","|  saptial_path                               |  0.371M                |  5.088G    |\n","|   saptial_path.convblock1                   |   1.856K               |   0.243G   |\n","|    saptial_path.convblock1.conv1            |    1.728K              |    0.226G  |\n","|    saptial_path.convblock1.bn               |    0.128K              |    16.777M |\n","|   saptial_path.convblock2                   |   73.984K              |   2.424G   |\n","|    saptial_path.convblock2.conv1            |    73.728K             |    2.416G  |\n","|    saptial_path.convblock2.bn               |    0.256K              |    8.389M  |\n","|   saptial_path.convblock3                   |   0.295M               |   2.42G    |\n","|    saptial_path.convblock3.conv1            |    0.295M              |    2.416G  |\n","|    saptial_path.convblock3.bn               |    0.512K              |    4.194M  |\n","|  context_path.features                      |  11.69M                |  19.002G   |\n","|   context_path.features.conv1               |   9.408K               |   1.233G   |\n","|    context_path.features.conv1.weight       |    (64, 3, 7, 7)       |            |\n","|   context_path.features.bn1                 |   0.128K               |   16.777M  |\n","|    context_path.features.bn1.weight         |    (64,)               |            |\n","|    context_path.features.bn1.bias           |    (64,)               |            |\n","|   context_path.features.layer1              |   0.148M               |   4.849G   |\n","|    context_path.features.layer1.0           |    73.984K             |    2.424G  |\n","|    context_path.features.layer1.1           |    73.984K             |    2.424G  |\n","|   context_path.features.layer2              |   0.526M               |   4.305G   |\n","|    context_path.features.layer2.0           |    0.23M               |    1.885G  |\n","|    context_path.features.layer2.1           |    0.295M              |    2.42G   |\n","|   context_path.features.layer3              |   2.1M                 |   4.3G     |\n","|    context_path.features.layer3.0           |    0.919M              |    1.882G  |\n","|    context_path.features.layer3.1           |    1.181M              |    2.418G  |\n","|   context_path.features.layer4              |   8.394M               |   4.298G   |\n","|    context_path.features.layer4.0           |    3.673M              |    1.881G  |\n","|    context_path.features.layer4.1           |    4.721M              |    2.417G  |\n","|   context_path.features.fc                  |   0.513M               |            |\n","|    context_path.features.fc.weight          |    (1000, 512)         |            |\n","|    context_path.features.fc.bias            |    (1000,)             |            |\n","|  attention_refinement_module1               |  66.304K               |  0.59M     |\n","|   attention_refinement_module1.conv         |   65.792K              |   65.536K  |\n","|    attention_refinement_module1.conv.weight |    (256, 256, 1, 1)    |            |\n","|    attention_refinement_module1.conv.bias   |    (256,)              |            |\n","|   attention_refinement_module1.bn           |   0.512K               |   0.512K   |\n","|    attention_refinement_module1.bn.weight   |    (256,)              |            |\n","|    attention_refinement_module1.bn.bias     |    (256,)              |            |\n","|   attention_refinement_module1.avgpool      |                        |   0.524M   |\n","|  attention_refinement_module2               |  0.264M                |  0.525M    |\n","|   attention_refinement_module2.conv         |   0.263M               |   0.262M   |\n","|    attention_refinement_module2.conv.weight |    (512, 512, 1, 1)    |            |\n","|    attention_refinement_module2.conv.bias   |    (512,)              |            |\n","|   attention_refinement_module2.bn           |   1.024K               |   1.024K   |\n","|    attention_refinement_module2.bn.weight   |    (512,)              |            |\n","|    attention_refinement_module2.bn.bias     |    (512,)              |            |\n","|   attention_refinement_module2.avgpool      |                        |   0.262M   |\n","|  supervision1                               |  4.883K                |            |\n","|   supervision1.weight                       |   (19, 256, 1, 1)      |            |\n","|   supervision1.bias                         |   (19,)                |            |\n","|  supervision2                               |  9.747K                |            |\n","|   supervision2.weight                       |   (19, 512, 1, 1)      |            |\n","|   supervision2.bias                         |   (19,)                |            |\n","|  feature_fusion_module                      |  0.176M                |  1.435G    |\n","|   feature_fusion_module.convblock           |   0.175M               |   1.435G   |\n","|    feature_fusion_module.convblock.conv1    |    0.175M              |    1.434G  |\n","|    feature_fusion_module.convblock.bn       |    38                  |    0.311M  |\n","|   feature_fusion_module.conv1               |   0.38K                |   0.361K   |\n","|    feature_fusion_module.conv1.weight       |    (19, 19, 1, 1)      |            |\n","|    feature_fusion_module.conv1.bias         |    (19,)               |            |\n","|   feature_fusion_module.conv2               |   0.38K                |   0.361K   |\n","|    feature_fusion_module.conv2.weight       |    (19, 19, 1, 1)      |            |\n","|    feature_fusion_module.conv2.bias         |    (19,)               |            |\n","|   feature_fusion_module.avgpool             |                        |   0.156M   |\n","|  conv                                       |  0.38K                 |  0.189G    |\n","|   conv.weight                               |   (19, 19, 1, 1)       |            |\n","|   conv.bias                                 |   (19,)                |            |\n"]}],"source":["# FLOPs\n","height, width = 512, 1024  # Sample input size\n","sample_input = torch.zeros((1, 3, height, width)).to(device)\n","flops = FlopCountAnalysis(bisenet_model, sample_input)\n","print(f\"FLOPs: {flop_count_table(flops)}\")"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T19:35:02.782163Z","iopub.status.busy":"2024-07-02T19:35:02.781311Z","iopub.status.idle":"2024-07-02T19:35:19.950476Z","shell.execute_reply":"2024-07-02T19:35:19.949561Z","shell.execute_reply.started":"2024-07-02T19:35:02.782129Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Mean Latency 17.143022537231445\n"," STD Latency 0.8200593395203266\n"," Mean FPS 58.458662595582865\n"," STD FPS : 2.877964082072746\n"]}],"source":["#Latency\n","height = 512\n","width = 1024\n","image = torch.rand(1, 3, height, width)\n","iterations = 1000\n","latency = []\n","FPS = []\n","for i in range(iterations):\n","    start = time.time()\n","    output = bisenet_model(image.to(device))\n","    end = time.time()\n","    l = end - start\n","    latency.append(l)\n","    fps = 1/l\n","    FPS.append(fps)\n","\n","meanLatency = np.mean(latency)*1000\n","stdLatency = np.std(latency)*1000\n","meanFPS = np.mean(FPS)\n","stdFPS = np.std(FPS)\n","\n","print(f'Mean Latency {meanLatency}\\n STD Latency {stdLatency}\\n Mean FPS {meanFPS}\\n STD FPS : {stdFPS}')"]},{"cell_type":"markdown","metadata":{},"source":["* Train: GTA5, Validation: Cityscapes"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T19:58:40.429835Z","iopub.status.busy":"2024-07-02T19:58:40.429490Z","iopub.status.idle":"2024-07-02T19:58:43.355061Z","shell.execute_reply":"2024-07-02T19:58:43.354259Z","shell.execute_reply.started":"2024-07-02T19:58:40.429809Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|██████████| 44.7M/44.7M [00:00<00:00, 140MB/s] \n","Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n","100%|██████████| 171M/171M [00:01<00:00, 159MB/s]  \n"]}],"source":["bisenet_model = BiSeNet(num_classes=num_classes, context_path='resnet18')"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T19:58:56.917017Z","iopub.status.busy":"2024-07-02T19:58:56.916187Z","iopub.status.idle":"2024-07-02T19:58:56.922515Z","shell.execute_reply":"2024-07-02T19:58:56.921485Z","shell.execute_reply.started":"2024-07-02T19:58:56.916983Z"},"trusted":true},"outputs":[],"source":["# Loss and optimizer\n","loss_fn = nn.CrossEntropyLoss(ignore_index=255).to(device)\n","optimizer = torch.optim.Adam(bisenet_model.parameters(), lr=1e-3)\n","init_lr = 1e-3"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T19:44:36.553375Z","iopub.status.busy":"2024-07-02T19:44:36.552461Z","iopub.status.idle":"2024-07-02T19:44:36.912484Z","shell.execute_reply":"2024-07-02T19:44:36.911505Z","shell.execute_reply.started":"2024-07-02T19:44:36.553333Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Training dataset size: 2500\n"]}],"source":["# Training Set: GTA5\n","root_dir = '/kaggle/input/gtadataset/GTA5'\n","\n","# Def of transformations for images and labels for training set\n","transform_img = transforms.Compose([\n","    transforms.Resize((720, 1280)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean = [0.485, 0.456, 0.406],\n","                          std = [0.229, 0.224, 0.225])\n","])\n","\n","def transform_label(label):\n","    label = F.resize(label, (720, 1280), interpolation=Image.NEAREST)\n","    label = np.array(label, dtype=np.int64)\n","    return label\n","\n","transform_label = transform_label\n","\n","# Dataset\n","train_dataset = GTA5(root_dir, split='train', transform_img=transform_img, transform_lab=transform_label)\n","batch_size = 4\n","\n","# DataLoader\n","train_loader_GTA5 = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","\n","# size\n","train_size = len(train_dataset)\n","print(\"Training dataset size:\", train_size)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["start_epoch = 0"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Upload the already trained model\n","'''\n","checkpoint = torch.load('/kaggle/input/project/MLDL2024_project1-master/gta_bisenet_final_checkpoint.pth')\n","bisenet_model.load_state_dict(checkpoint['model_state_dict'])\n","optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","last_lr = checkpoint['lr']\n","start_epoch = checkpoint['epoch'] + 1\n","optimizer = torch.optim.Adam(bisenet_model.parameters(), lr=1e-3)\n","print(start_epoch)\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["bisenet_model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Training\n","for i in range(start_epoch,50):\n","    current_lr = poly_lr_scheduler(optimizer, init_lr, iter = i)\n","    print(f'Epoch: {i}')\n","    train(bisenet_model, optimizer, train_loader_GTA5, loss_fn,device)\n","    print(f\"Current Learning Rate: {current_lr:.6f}\")\n","    torch.save({\n","          'model_state_dict' : bisenet_model.state_dict(),\n","          'optimizer_state_dict' : optimizer.state_dict(),\n","          'epoch' : i,\n","          'lr': current_lr\n","      }, '/kaggle/working/bisenet_checkpoint.pth')"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T19:59:40.865532Z","iopub.status.busy":"2024-07-02T19:59:40.864707Z","iopub.status.idle":"2024-07-02T19:59:40.877628Z","shell.execute_reply":"2024-07-02T19:59:40.876731Z","shell.execute_reply.started":"2024-07-02T19:59:40.865499Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Validation dataset size: 500\n"]}],"source":["# Validation Set: Cityscapes\n","root_dir_val = '/kaggle/input/citiscapes/Cityscapes/Cityspaces'\n","\n","# Def of transformations for images and labels for validation set\n","transform_img = transforms.Compose([\n","    transforms.Resize((512, 1024)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean = [0.485, 0.456, 0.406],\n","                         std = [0.229, 0.224, 0.225])\n","\n","])\n","\n","def transform_label(label):\n","    label = F.resize(label, (512, 1024), interpolation=Image.NEAREST)\n","    label = np.array(label, dtype=np.int64)\n","    return label\n","transform_label = transform_label\n","batch_size=4\n","# Dataset\n","val_dataset = CityScapes(root_dir_val, split='val', transform_img=transform_img,transform_lab=transform_label)\n","# Dataloader\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=2)\n","\n","# size\n","val_size = len(val_dataset)\n","print(\"Validation dataset size:\", val_size)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T19:59:44.041163Z","iopub.status.busy":"2024-07-02T19:59:44.040326Z","iopub.status.idle":"2024-07-02T20:00:26.869705Z","shell.execute_reply":"2024-07-02T20:00:26.868625Z","shell.execute_reply.started":"2024-07-02T19:59:44.041130Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'Final mIoU:': 0.15122540502980422}\n","road : 8.3543%\n","sidewalk : 1.2864%\n","building : 37.4049%\n","wall : 1.7961%\n","fence : 6.4966%\n","pole : 12.8021%\n","light : 13.9396%\n","sign : 2.2340%\n","vegetation : 73.4990%\n","terrain : 12.7513%\n","sky : 57.9718%\n","person : 23.6138%\n","rider : 0.0000%\n","car : 30.0205%\n","truck : 2.1330%\n","bus : 3.0157%\n","train : 0.0093%\n","motocycle : 0.0000%\n","bicycle : 0.0000%\n"]}],"source":["# Validation\n","classes = ['road', 'sidewalk', 'building', 'wall', 'fence', 'pole', 'light', 'sign','vegetation', 'terrain', 'sky','person',\n","        'rider', 'car','truck','bus','train', 'motocycle','bicycle'] \n","\n","val = val(bisenet_model, val_loader, device)\n","for i in range(len(classes)) : \n","    print(f'{classes[i]} : {val[i]*100:.4f}%')"]},{"cell_type":"markdown","metadata":{},"source":["# BISENET AUG1 (HORIZONTAL FLIP)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:02:33.711279Z","iopub.status.busy":"2024-07-02T20:02:33.710508Z","iopub.status.idle":"2024-07-02T20:02:34.819820Z","shell.execute_reply":"2024-07-02T20:02:34.818686Z","shell.execute_reply.started":"2024-07-02T20:02:33.711247Z"},"trusted":true},"outputs":[],"source":["bisenet_model = BiSeNet(num_classes=num_classes, context_path='resnet18')"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:02:37.960930Z","iopub.status.busy":"2024-07-02T20:02:37.960586Z","iopub.status.idle":"2024-07-02T20:02:37.987952Z","shell.execute_reply":"2024-07-02T20:02:37.986996Z","shell.execute_reply.started":"2024-07-02T20:02:37.960906Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using 2 GPUs!\n"]}],"source":["# Multiple GPU \n","device= \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","bisenet_model=bisenet_model.to(device)\n","if torch.cuda.device_count()>1:\n","    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n","    bisenet_model=nn.DataParallel(bisenet_model)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:02:40.651826Z","iopub.status.busy":"2024-07-02T20:02:40.651480Z","iopub.status.idle":"2024-07-02T20:02:40.658029Z","shell.execute_reply":"2024-07-02T20:02:40.656999Z","shell.execute_reply.started":"2024-07-02T20:02:40.651799Z"},"trusted":true},"outputs":[],"source":["# Loss and optimizer\n","loss_fn = nn.CrossEntropyLoss(ignore_index=255).to(device)\n","optimizer = torch.optim.Adam(bisenet_model.parameters(), lr=1e-3)\n","init_lr = 1e-3"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T19:50:50.561531Z","iopub.status.busy":"2024-07-02T19:50:50.560697Z","iopub.status.idle":"2024-07-02T19:50:50.707656Z","shell.execute_reply":"2024-07-02T19:50:50.706677Z","shell.execute_reply.started":"2024-07-02T19:50:50.561495Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Training dataset size: 2500\n"]}],"source":["# Training Set: GTA5\n","root_dir = '/kaggle/input/gtadataset/GTA5'\n","\n","# Def of transformations for images and labels for training set\n","transform_img = transforms.Compose([\n","    transforms.Resize((720, 1280)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean = [0.485, 0.456, 0.406],\n","                          std = [0.229, 0.224, 0.225])\n","])\n","\n","def transform_label(label):\n","    label = F.resize(label, (720, 1280), interpolation=Image.NEAREST)\n","    label = np.array(label, dtype=np.int64)\n","    return label\n","\n","# Augumentation\n","def transform_paired(image, label) :\n","    random_aug = random.random()\n","    random_Crop_or_Flip = random.random()\n","    if random_aug < 0.5: \n","        image = F.hflip(image)\n","        label = F.hflip(label)\n","    \n","    return image, label\n","\n","transform_label = transform_label\n","transform_paired = transform_paired\n","\n","# Dataset\n","train_dataset = GTA5(root_dir, split='train', transform_img=transform_img, transform_lab=transform_label,transform_paired=transform_paired)\n","batch_size = 8\n","\n","# DataLoader\n","train_loader_GTA5 = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","\n","# size\n","train_size = len(train_dataset)\n","print(\"Training dataset size:\", train_size)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["start_epoch = 0"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Upload the already trained model\n","'''\n","checkpoint = torch.load('/kaggle/input/project/MLDL2024_project1-master/gta_bisenet_aug1_final_checkpoint.pth')\n","bisenet_model.load_state_dict(checkpoint['model_state_dict'])\n","optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","last_lr = checkpoint['lr']\n","start_epoch = checkpoint['epoch'] + 1\n","optimizer = torch.optim.Adam(bisenet_model.parameters(), lr=1e-3)\n","print(\"Start Epoch:\", start_epoch)\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Training\n","for i in range(start_epoch, 50):\n","    #virtually expanded\n","    train_dataset = GTA5(root_dir, split='train', transform_img=transform_img, transform_lab=transform_label,transform_paired=transform_paired)\n","    train_loader_GTA5 = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","\n","    current_lr = poly_lr_scheduler(optimizer, init_lr, iter = i)\n","    print(f'Epoch: {i}')\n","    train(bisenet_model, optimizer, train_loader_GTA5, loss_fn, device, is_gta5 = True)\n","    print(f\"Current Learning Rate: {current_lr:.6f}\")\n","    torch.save({\n","      'model_state_dict' : bisenet_model.state_dict(),\n","      'optimizer_state_dict' : optimizer.state_dict(),\n","      'epoch' : i,\n","      'lr': current_lr\n","    }, '/kaggle/working/gta_bisenet_aug1_nocrop_checkpoint_50.pth')"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T19:51:04.654078Z","iopub.status.busy":"2024-07-02T19:51:04.653667Z","iopub.status.idle":"2024-07-02T19:51:04.738267Z","shell.execute_reply":"2024-07-02T19:51:04.737257Z","shell.execute_reply.started":"2024-07-02T19:51:04.654044Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Validation dataset size: 500\n"]}],"source":["# Validation Set: Cityscapes\n","root_dir_val = '/kaggle/input/citiscapes/Cityscapes/Cityspaces'\n","\n","# Def of transformations for images and labels for validation set\n","transform_img = transforms.Compose([\n","    transforms.Resize((512, 1024)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean = [0.485, 0.456, 0.406],\n","                         std = [0.229, 0.224, 0.225])\n","\n","])\n","\n","def transform_label(label):\n","    label = F.resize(label, (512, 1024), interpolation=Image.NEAREST)\n","    label = np.array(label, dtype=np.int64)\n","    return label\n","transform_label = transform_label\n","\n","# Dataset\n","val_dataset = CityScapes(root_dir_val, split='val', transform_img=transform_img,transform_lab=transform_label)\n","# Dataloader\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=2)\n","\n","# size\n","val_size = len(val_dataset)\n","print(\"Validation dataset size:\", val_size)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T19:51:11.109220Z","iopub.status.busy":"2024-07-02T19:51:11.108553Z","iopub.status.idle":"2024-07-02T19:52:01.321344Z","shell.execute_reply":"2024-07-02T19:52:01.320034Z","shell.execute_reply.started":"2024-07-02T19:51:11.109187Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'Final mIoU:': 0.1757125084072001}\n","road : 34.0390%\n","sidewalk : 4.0214%\n","building : 44.9786%\n","wall : 9.6315%\n","fence : 4.1427%\n","pole : 13.8076%\n","light : 10.4628%\n","sign : 1.9014%\n","vegetation : 70.6853%\n","terrain : 6.9042%\n","sky : 61.6067%\n","person : 29.7800%\n","rider : 0.4139%\n","car : 36.4590%\n","truck : 3.9542%\n","bus : 0.4199%\n","train : 0.0000%\n","motocycle : 0.6434%\n","bicycle : 0.0022%\n"]}],"source":["# Validation\n","classes = ['road', 'sidewalk', 'building', 'wall', 'fence', 'pole', 'light', 'sign','vegetation', 'terrain', 'sky','person',\n","        'rider', 'car','truck','bus','train', 'motocycle','bicycle'] \n","\n","val = val(bisenet_model, val_loader, device)\n","for i in range(len(classes)) : \n","    print(f'{classes[i]} : {val[i]*100:.4f}%')"]},{"cell_type":"markdown","metadata":{},"source":["# BISENET AUG2 (VISUAL TRASFORMATIONS)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:21:07.268288Z","iopub.status.busy":"2024-07-02T20:21:07.267612Z","iopub.status.idle":"2024-07-02T20:21:10.229268Z","shell.execute_reply":"2024-07-02T20:21:10.228504Z","shell.execute_reply.started":"2024-07-02T20:21:07.268257Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|██████████| 44.7M/44.7M [00:00<00:00, 134MB/s]\n","Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n","100%|██████████| 171M/171M [00:01<00:00, 166MB/s]  \n"]}],"source":["bisenet_model = BiSeNet(num_classes=num_classes, context_path='resnet18')"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:21:12.151484Z","iopub.status.busy":"2024-07-02T20:21:12.150629Z","iopub.status.idle":"2024-07-02T20:21:12.405457Z","shell.execute_reply":"2024-07-02T20:21:12.404559Z","shell.execute_reply.started":"2024-07-02T20:21:12.151446Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using 2 GPUs!\n"]}],"source":["# Multiple GPU \n","device= \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","bisenet_model=bisenet_model.to(device)\n","if torch.cuda.device_count()>1:\n","    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n","    bisenet_model=nn.DataParallel(bisenet_model)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:21:14.532880Z","iopub.status.busy":"2024-07-02T20:21:14.532508Z","iopub.status.idle":"2024-07-02T20:21:14.538973Z","shell.execute_reply":"2024-07-02T20:21:14.537957Z","shell.execute_reply.started":"2024-07-02T20:21:14.532853Z"},"trusted":true},"outputs":[],"source":["# Loss and optimizer\n","loss_fn = nn.CrossEntropyLoss(ignore_index=255).to(device)\n","optimizer = torch.optim.Adam(bisenet_model.parameters(), lr=1e-3)\n","init_lr = 1e-3"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:21:18.151972Z","iopub.status.busy":"2024-07-02T20:21:18.151609Z","iopub.status.idle":"2024-07-02T20:21:18.479869Z","shell.execute_reply":"2024-07-02T20:21:18.478941Z","shell.execute_reply.started":"2024-07-02T20:21:18.151944Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Training dataset size: 2500\n"]}],"source":["# Training Set: GTA5\n","root_dir = '/kaggle/input/gtadataset/GTA5'\n","\n","class ColorJitter(transforms.ColorJitter):\n","    def __call__(self, image, target):\n","        return super().__call__(image), target    \n","\n","# Augmentation\n","transform_img = transforms.Compose([\n","    transforms.Resize((720, 1280)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean = [0.485, 0.456, 0.406],\n","                          std = [0.229, 0.224, 0.225]),\n","    transforms.RandomApply([transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05)], p=0.5),\n","    transforms.RandomApply([transforms.GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 0.1))], p=0.5),\n","    transforms.RandomApply([transforms.Lambda(lambda img: img + torch.randn_like(img) * 0.02)], p=0.5), \n","    #transforms.RandomApply([transforms.Lambda(lambda img: transforms.functional.adjust_gamma(img, gamma=0.8))], p=0.5), #gives problems with gaussian blur\n","])\n","\n","# Def of transformations for labels for training set\n","def transform_label(label):\n","    label = F.resize(label, (720, 1280), interpolation=Image.NEAREST)\n","    label = np.array(label, dtype=np.int64)\n","    return label\n","\n","transform_label = transform_label\n","\n","#dataset\n","train_dataset = GTA5(root_dir, split='train', transform_img=transform_img, transform_lab=transform_label)\n","batch_size = 8\n","\n","# DataLoader\n","train_loader_GTA5 = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","\n","# size\n","train_size = len(train_dataset)\n","print(\"Training dataset size:\", train_size)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["start_epoch = 0"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#Upload the already trained model\n","'''\n","checkpoint = torch.load('/kaggle/input/project/MLDL2024_project1-master/gta_bisenet_aug2_final_checkpoint.pth')\n","bisenet_model.load_state_dict(checkpoint['model_state_dict'])\n","optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","last_lr = checkpoint['lr']\n","start_epoch = checkpoint['epoch'] + 1\n","optimizer = torch.optim.Adam(bisenet_model.parameters(), lr=1e-3)\n","print(start_epoch)\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Training\n","for i in range(start_epoch, 50):\n","    #virtually expanded\n","    train_dataset = GTA5(root_dir, split='train', transform_img=transform_img, transform_lab=transform_label)\n","    train_loader_GTA5 = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","\n","    current_lr = poly_lr_scheduler(optimizer, init_lr, iter = i)\n","    print(f'Epoch: {i}')\n","    train(bisenet_model, optimizer, train_loader_GTA5, loss_fn, device, is_gta5 = True)\n","    print(f\"Current Learning Rate: {current_lr:.6f}\")\n","    torch.save({\n","          'model_state_dict' : bisenet_model.state_dict(),\n","          'optimizer_state_dict' : optimizer.state_dict(),\n","          'epoch' : i,\n","          'lr': current_lr\n","        }, '/kaggle/working/gta_bisenet_aug2_checkpoint.pth')      "]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:21:30.135255Z","iopub.status.busy":"2024-07-02T20:21:30.134883Z","iopub.status.idle":"2024-07-02T20:21:30.258385Z","shell.execute_reply":"2024-07-02T20:21:30.257397Z","shell.execute_reply.started":"2024-07-02T20:21:30.135225Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Validation dataset size: 500\n"]}],"source":["# Validation Set: Cityscapes\n","root_dir_val = '/kaggle/input/citiscapes/Cityscapes/Cityspaces'\n","\n","# Def of transformations for images and labels for validation set\n","transform_img = transforms.Compose([\n","    transforms.Resize((512, 1024)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean = [0.485, 0.456, 0.406],\n","                          std = [0.229, 0.224, 0.225])\n","\n","])\n","\n","def transform_label(label):\n","    label = F.resize(label, (512, 1024), interpolation=Image.NEAREST)\n","    label = np.array(label, dtype=np.int64)\n","    return label\n","transform_label = transform_label\n","\n","#Dataset\n","val_dataset = CityScapes(root_dir_val, split='val', transform_img=transform_img,transform_lab=transform_label)\n","#Dataloader\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=2)\n","# size\n","val_size = len(val_dataset)\n","print(\"Validation dataset size:\", val_size)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:21:33.410839Z","iopub.status.busy":"2024-07-02T20:21:33.410160Z","iopub.status.idle":"2024-07-02T20:22:19.150616Z","shell.execute_reply":"2024-07-02T20:22:19.149599Z","shell.execute_reply.started":"2024-07-02T20:21:33.410805Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'Final mIoU:': 0.21005380173728802}\n","road : 31.0664%\n","sidewalk : 16.2279%\n","building : 61.8424%\n","wall : 9.9058%\n","fence : 5.2766%\n","pole : 20.3691%\n","light : 19.4529%\n","sign : 4.8772%\n","vegetation : 72.8250%\n","terrain : 5.7645%\n","sky : 80.6103%\n","person : 30.0358%\n","rider : 0.1447%\n","car : 28.8470%\n","truck : 10.3221%\n","bus : 0.0067%\n","train : 0.0000%\n","motocycle : 1.5280%\n","bicycle : 0.0000%\n"]}],"source":["# Validation\n","classes = ['road', 'sidewalk', 'building', 'wall', 'fence','pole', 'light', 'sign','vegetation', 'terrain', 'sky','person',\n","        'rider', 'car','truck','bus','train', 'motocycle','bicycle'] \n","\n","val = val(bisenet_model, val_loader, device)\n","for i in range(len(classes)) : \n","    print(f'{classes[i]} : {val[i]*100:.4f}%')"]},{"cell_type":"markdown","metadata":{},"source":["# BISENET AUG1 + AUG2"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:25:18.608431Z","iopub.status.busy":"2024-07-02T20:25:18.607730Z","iopub.status.idle":"2024-07-02T20:25:21.980088Z","shell.execute_reply":"2024-07-02T20:25:21.978704Z","shell.execute_reply.started":"2024-07-02T20:25:18.608392Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|██████████| 44.7M/44.7M [00:00<00:00, 129MB/s] \n","Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n","100%|██████████| 171M/171M [00:01<00:00, 134MB/s]  \n"]}],"source":["bisenet_model = BiSeNet(num_classes=num_classes, context_path='resnet18')"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:22:40.153009Z","iopub.status.busy":"2024-07-02T20:22:40.152623Z","iopub.status.idle":"2024-07-02T20:22:40.179954Z","shell.execute_reply":"2024-07-02T20:22:40.179121Z","shell.execute_reply.started":"2024-07-02T20:22:40.152979Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using 2 GPUs!\n"]}],"source":["# Multiple GPU\n","device= \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","bisenet_model=bisenet_model.to(device)\n","if torch.cuda.device_count()>1:\n","    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n","    bisenet_model=nn.DataParallel(bisenet_model)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:25:24.403533Z","iopub.status.busy":"2024-07-02T20:25:24.399899Z","iopub.status.idle":"2024-07-02T20:25:24.413935Z","shell.execute_reply":"2024-07-02T20:25:24.412792Z","shell.execute_reply.started":"2024-07-02T20:25:24.403460Z"},"trusted":true},"outputs":[],"source":["# Loss and optimizer\n","loss_fn = nn.CrossEntropyLoss(ignore_index=255).to(device)\n","optimizer = torch.optim.Adam(bisenet_model.parameters(), lr=1e-3)\n","init_lr = 1e-3"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:25:31.373865Z","iopub.status.busy":"2024-07-02T20:25:31.373487Z","iopub.status.idle":"2024-07-02T20:25:31.495018Z","shell.execute_reply":"2024-07-02T20:25:31.493907Z","shell.execute_reply.started":"2024-07-02T20:25:31.373837Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Training dataset size: 2500\n"]}],"source":["# Training Set: GTA5\n","root_dir = '/kaggle/input/gtadataset/GTA5'\n","\n","# Def of transformations for images and labels for training set\n","transform_img = transforms.Compose([\n","    transforms.Resize((720, 1280)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean = [0.485, 0.456, 0.406],\n","                          std = [0.229, 0.224, 0.225])\n","\n","])\n","\n","def transform_label(label):\n","    label = F.resize(label, (720, 1280), interpolation=Image.NEAREST)\n","    label = np.array(label, dtype=np.int64)\n","    return label\n","\n","#Augmentation\n","def transform_paired(image, label):\n","    random_aug = random.random()\n","\n","    if random_aug < 0.5:\n","        image = F.hflip(image)\n","        label = F.hflip(label)\n","        \n","        image = transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05)(image)\n","        image = transforms.GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 0.1))(image)\n","        image = transforms.Lambda(lambda img: img + torch.randn_like(img) * 0.02)(image)\n","   \n","    return image, label\n","\n","transform_label = transform_label\n","transform_paired = transform_paired\n","\n","# Dataset\n","train_dataset = GTA5(root_dir, split='train', transform_img=transform_img, transform_lab=transform_label,transform_paired=transform_paired)\n","batch_size = 8\n","# DataLoader\n","train_loader_GTA5 = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","\n","# size\n","train_size = len(train_dataset)\n","print(\"Training dataset size:\", train_size)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["start_epoch = 0"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Upload the already trained model\n","'''\n","checkpoint = torch.load('/kaggle/input/project/MLDL2024_project1-master/gta_bisenet_aug12_final_checkpoint.pth')\n","bisenet_model.load_state_dict(checkpoint['model_state_dict'])\n","optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","last_lr = checkpoint['lr']\n","start_epoch = checkpoint['epoch'] + 1\n","optimizer = torch.optim.Adam(bisenet_model.parameters(), lr=1e-3)\n","print(\"Start Epoch:\", start_epoch)\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Training\n","for i in range(start_epoch, 50):\n","    #virtually expanded\n","    train_dataset = GTA5(root_dir, split='train', transform_img=transform_img, transform_lab=transform_label,transform_paired=transform_paired)\n","    train_loader_GTA5 = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    \n","    current_lr = poly_lr_scheduler(optimizer, init_lr, iter = i)\n","    print(f'Epoch: {i}')\n","    train(bisenet_model, optimizer, train_loader_GTA5, loss_fn,device, is_gta5 = True)\n","    print(f\"Current Learning Rate: {current_lr:.6f}\")\n","    torch.save({\n","      'model_state_dict' : bisenet_model.state_dict(),\n","      'optimizer_state_dict' : optimizer.state_dict(),\n","      'epoch' : i,\n","      'lr': current_lr\n","    }, '/kaggle/working/gta_bisenet_aug12_checkpoint.pth')"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:25:44.101350Z","iopub.status.busy":"2024-07-02T20:25:44.100652Z","iopub.status.idle":"2024-07-02T20:25:44.183480Z","shell.execute_reply":"2024-07-02T20:25:44.182355Z","shell.execute_reply.started":"2024-07-02T20:25:44.101316Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Validation dataset size: 500\n"]}],"source":["# Validation Set: Cityscapes\n","root_dir_val = '/kaggle/input/citiscapes/Cityscapes/Cityspaces'\n","\n","# Def of transformations for images and labels for validation set\n","transform_img = transforms.Compose([\n","    transforms.Resize((512, 1024)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean = [0.485, 0.456, 0.406],\n","                          std = [0.229, 0.224, 0.225])\n","])\n","\n","def transform_label(label):\n","    label = F.resize(label, (512, 1024), interpolation=Image.NEAREST)\n","    label = np.array(label, dtype=np.int64)\n","    return label\n","transform_label = transform_label\n","\n","# Dataset\n","val_dataset = CityScapes(root_dir_val, split='val', transform_img=transform_img,transform_lab=transform_label)\n","# Dataloader\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=2)\n","\n","# size\n","val_size = len(val_dataset)\n","print(\"Validation dataset size:\", val_size)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:28:18.998309Z","iopub.status.busy":"2024-07-02T20:28:18.997598Z","iopub.status.idle":"2024-07-02T20:29:05.826411Z","shell.execute_reply":"2024-07-02T20:29:05.825071Z","shell.execute_reply.started":"2024-07-02T20:28:18.998274Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'Final mIoU:': 0.23138683399844664}\n","road : 42.5805%\n","sidewalk : 13.0382%\n","building : 71.3595%\n","wall : 16.1009%\n","fence : 10.0409%\n","pole : 17.3282%\n","light : 16.6782%\n","sign : 4.9166%\n","vegetation : 77.3120%\n","terrain : 17.5203%\n","sky : 80.6919%\n","person : 33.4542%\n","rider : 1.2103%\n","car : 20.4097%\n","truck : 9.5867%\n","bus : 2.8110%\n","train : 0.0000%\n","motocycle : 4.5907%\n","bicycle : 0.0050%\n"]}],"source":["# Validation\n","classes = ['road', 'sidewalk', 'building', 'wall', 'fence', 'pole', 'light', 'sign','vegetation', 'terrain', 'sky','person',\n","        'rider', 'car','truck','bus','train', 'motocycle','bicycle'] \n","\n","val = val(bisenet_model, val_loader, device)\n","for i in range(len(classes)) : \n","    print(f'{classes[i]} : {val[i]*100:.4f}%')"]},{"cell_type":"markdown","metadata":{},"source":["# FDA"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:37:08.140966Z","iopub.status.busy":"2024-07-02T20:37:08.140124Z","iopub.status.idle":"2024-07-02T20:37:11.137838Z","shell.execute_reply":"2024-07-02T20:37:11.137056Z","shell.execute_reply.started":"2024-07-02T20:37:08.140934Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|██████████| 44.7M/44.7M [00:00<00:00, 140MB/s] \n","Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n","100%|██████████| 171M/171M [00:01<00:00, 145MB/s]  \n"]}],"source":["bisenet_model = BiSeNet(num_classes=num_classes, context_path='resnet18')"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:37:12.525516Z","iopub.status.busy":"2024-07-02T20:37:12.524736Z","iopub.status.idle":"2024-07-02T20:37:12.765362Z","shell.execute_reply":"2024-07-02T20:37:12.764486Z","shell.execute_reply.started":"2024-07-02T20:37:12.525487Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using 2 GPUs!\n"]}],"source":["#Multiple GPU\n","device= \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","bisenet_model=bisenet_model.to(device)\n","if torch.cuda.device_count()>1:\n","    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n","    bisenet_model=nn.DataParallel(bisenet_model) "]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:38:04.044888Z","iopub.status.busy":"2024-07-02T20:38:04.044057Z","iopub.status.idle":"2024-07-02T20:38:04.050380Z","shell.execute_reply":"2024-07-02T20:38:04.049432Z","shell.execute_reply.started":"2024-07-02T20:38:04.044855Z"},"trusted":true},"outputs":[],"source":["# Loss and optimizer\n","loss_fn = nn.CrossEntropyLoss(ignore_index=255).to(device)\n","optimizer = torch.optim.Adam(bisenet_model.parameters(), lr=1e-3)\n","init_lr = 1e-3"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:38:09.735951Z","iopub.status.busy":"2024-07-02T20:38:09.735203Z","iopub.status.idle":"2024-07-02T20:38:09.901196Z","shell.execute_reply":"2024-07-02T20:38:09.900261Z","shell.execute_reply.started":"2024-07-02T20:38:09.735919Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Training dataset size: 2500\n"]}],"source":["# Training Set: GTA5\n","root_dir = '/kaggle/input/gtadataset/GTA5'\n","\n","transform_img = transforms.Compose([\n","    transforms.Resize((720, 1280)),\n","    transforms.ToTensor(),\n","])\n","\n","def transform_label(label):\n","    label = F.resize(label, (720, 1280), interpolation=Image.NEAREST)\n","    label = np.array(label, dtype=np.int64)\n","    return label\n","\n","def transform_paired(image, label):\n","    random_aug = random.random()\n","\n","    if random_aug < 0.5:\n","        image = F.hflip(image)\n","        label = F.hflip(label)\n","\n","    return image, label\n","\n","transform_label = transform_label\n","transform_paired = transform_paired\n","\n","train_dataset = GTA5(root_dir, split='train', transform_img=transform_img, transform_lab=transform_label, transform_paired=transform_paired)\n","batch_size = 4\n","\n","# Create a DataLoader for the training dataset\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","train_size = len(train_dataset)\n","\n","print(\"Training dataset size:\", train_size)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:38:14.711778Z","iopub.status.busy":"2024-07-02T20:38:14.711409Z","iopub.status.idle":"2024-07-02T20:38:15.225781Z","shell.execute_reply":"2024-07-02T20:38:15.225000Z","shell.execute_reply.started":"2024-07-02T20:38:14.711745Z"},"trusted":true},"outputs":[],"source":["# Cityscapes Dataset\n","root_dir_val = '/kaggle/input/citiscapes/Cityscapes/Cityspaces'\n","\n","City_dataset = CityScapes(root_dir_val, split='train', transform_img=transform_img,transform_lab=transform_label)\n","City_loader = DataLoader(City_dataset, batch_size=4, num_workers=2)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:38:22.771970Z","iopub.status.busy":"2024-07-02T20:38:22.771611Z","iopub.status.idle":"2024-07-02T20:38:22.783070Z","shell.execute_reply":"2024-07-02T20:38:22.781972Z","shell.execute_reply.started":"2024-07-02T20:38:22.771942Z"},"trusted":true},"outputs":[],"source":["# FDA Functions\n","def low_freq_mutate_np( amp_src, amp_trg, L=0.1 ):\n","    a_src = np.fft.fftshift( amp_src, axes=(-2, -1) )\n","    a_trg = np.fft.fftshift( amp_trg, axes=(-2, -1) )\n","\n","    _, h, w = a_src.shape\n","    b = (  np.floor(np.amin((h,w))*L)  ).astype(int)\n","    c_h = np.floor(h/2.0).astype(int)\n","    c_w = np.floor(w/2.0).astype(int)\n","\n","    h1 = c_h-b\n","    h2 = c_h+b+1\n","    w1 = c_w-b\n","    w2 = c_w+b+1\n","\n","    a_src[:,h1:h2,w1:w2] = a_trg[:,h1:h2,w1:w2]\n","    a_src = np.fft.ifftshift( a_src, axes=(-2, -1) )\n","    return a_src\n","\n","def FDA_source_to_target_np( src_img, trg_img, L=0.1 ):\n","    src_img_np = src_img \n","    trg_img_np = trg_img \n","\n","    fft_src_np = np.fft.fft2( src_img_np, axes=(-2, -1) )\n","    fft_trg_np = np.fft.fft2( trg_img_np, axes=(-2, -1) )\n","\n","    amp_src, pha_src = np.abs(fft_src_np), np.angle(fft_src_np)\n","    amp_trg, pha_trg = np.abs(fft_trg_np), np.angle(fft_trg_np)\n","\n","    amp_src_ = low_freq_mutate_np( amp_src, amp_trg, L=L )\n","\n","    fft_src_ = amp_src_ * np.exp( 1j * pha_src )\n","\n","    src_in_trg = np.fft.ifft2( fft_src_, axes=(-2, -1) )\n","    src_in_trg = np.real(src_in_trg)\n","\n","    return src_in_trg"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["start_epoch = 0"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:38:41.542107Z","iopub.status.busy":"2024-07-02T20:38:41.541770Z","iopub.status.idle":"2024-07-02T20:38:41.561004Z","shell.execute_reply":"2024-07-02T20:38:41.560089Z","shell.execute_reply.started":"2024-07-02T20:38:41.542081Z"},"trusted":true},"outputs":[],"source":["# FDA Transform & DACS Training Functions\n","def FDA_Transform(src_img_batch, src_lbl_batch, trg_img_batch, trg_lbl_batch) : \n","    transform_aug = transforms.Compose([\n","        transforms.RandomApply([transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05)], p=0.5),\n","        transforms.RandomApply([transforms.GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 0.1))], p=0.5),\n","        transforms.RandomApply([transforms.Lambda(lambda img: img + torch.randn_like(img) * 0.02)], p=0.5), \n","    ])\n","\n","    src_img_batch = np.asarray(src_img_batch.cpu(), np.float32)\n","    trg_img_batch = np.asarray(trg_img_batch.cpu(), np.float32)\n","    src_in_trg_batch = np.array([FDA_source_to_target_np(src_img, trg_img, L=0.01) for src_img, trg_img in zip(src_img_batch, trg_img_batch)])\n","    \n","    src_in_trg_tensor = torch.from_numpy(src_in_trg_batch)\n","    src_in_trg_tensor = transform_aug(src_in_trg_tensor)\n","    src_in_trg_tensor = src_in_trg_tensor.numpy()\n","\n","    src_in_trg_tensor = src_in_trg_tensor.transpose((0, 2, 3, 1))\n","    src_in_trg_batch = src_in_trg_batch.transpose((0, 2, 3, 1))\n","    trg_img_batch = trg_img_batch.transpose((0, 2, 3, 1))\n","    src_img_batch = src_img_batch.transpose((0, 2, 3, 1))\n","    \n","    return src_in_trg_tensor, src_in_trg_batch, trg_img_batch, src_img_batch \n","\n","def train_FDA(model, optimizer, dataloader_GTA, val_loader, loss_fn, device, is_gta5=False):\n","    model.train()\n","    hist = np.zeros((19, 19))\n","\n","    IMG_MEAN = np.array((104.00698793, 116.66876762, 122.67891434), dtype=np.float32)\n","    IMG_MEAN = torch.reshape(torch.from_numpy(IMG_MEAN), (1, 3, 1, 1))\n","    mean_img = torch.zeros(1, 1)\n","\n","    val_iter = iter(val_loader)\n","\n","    for batch_idx, (src_img_batch, src_lbl_batch) in enumerate(dataloader_GTA):\n","        try:\n","            trg_img_batch, trg_lbl_batch = next(val_iter)\n","        except StopIteration:\n","            val_iter = iter(val_loader)\n","            trg_img_batch, trg_lbl_batch = next(val_iter)\n","            \n","        src_img_batch, src_lbl_batch = src_img_batch.to(device), src_lbl_batch.to(device).long()\n","        trg_img_batch, trg_lbl_batch = trg_img_batch.to(device), trg_lbl_batch.to(device).long()\n","        \n","        if is_gta5: \n","            src_lbl_batch[src_lbl_batch > 18] = 255\n","            \n","        src_in_trg_tensor, src_in_trg_batch, trg_img_batch, src_img_batch  = FDA_Transform(src_img_batch, src_lbl_batch, trg_img_batch, trg_lbl_batch)\n","        src_in_trg_tensor = src_in_trg_tensor.transpose((0, 3, 1, 2))\n","        src_in_trg_tensor = torch.tensor(src_in_trg_tensor, dtype=torch.float32)\n","        \n","        normalize = transforms.Normalize(mean=mean, std=std)\n","        src_in_trg_tensor = normalize(src_in_trg_tensor)\n","        \n","        outputs = model(src_in_trg_tensor)\n","        src_lbl_batch = src_lbl_batch.to(outputs[0].device)\n","        loss = loss_fn(outputs[0], src_lbl_batch)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        _, predicted = outputs[0].max(1)\n","        \n","        if batch_idx % 100 == 0 : \n","            hist += fast_hist(src_lbl_batch.cpu().flatten().numpy(), predicted.cpu().flatten().numpy(), 19)\n","            print(per_class_iou(hist))\n","            miou = np.mean(per_class_iou(hist)) \n","            print(f'Partial mIoU at batch {batch_idx} = {miou}') \n","            \n","        hist += fast_hist(src_lbl_batch.cpu().flatten().numpy(), predicted.cpu().flatten().numpy(), 19)\n","        miou = np.mean(per_class_iou(hist)) \n"," \n","\n","    print(per_class_iou(hist))\n","    miou = np.mean(per_class_iou(hist))\n","    print({\"Final mIoU:\": miou})"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Upload the already trained model\n","'''\n","checkpoint = torch.load('/kaggle/input/project/MLDL2024_project1-master/gta_bisenet_FDA_checkpoint_final.pth')\n","bisenet_model.load_state_dict(checkpoint['model_state_dict'], strict = False)\n","optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","last_lr = checkpoint['lr']\n","start_epoch = checkpoint['epoch'] + 1\n","optimizer = torch.optim.Adam(bisenet_model.parameters(), lr=1e-3)\n","print(\"Start Epoch:\", start_epoch)\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Training\n","for i in range(start_epoch, 50):\n","    train_dataset = GTA5(root_dir, split='train', transform_img=transform_img, transform_lab=transform_label,transform_paired=transform_paired)\n","    train_loader_GTA5 = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","\n","    current_lr = poly_lr_scheduler(optimizer, init_lr, iter = i)\n","    print(f'Epoch: {i}')\n","    train_FDA(bisenet_model, optimizer, train_loader_GTA5, City_loader, loss_fn, device, is_gta5 = True)\n","    torch.save({\n","      'model_state_dict' : bisenet_model.state_dict(),\n","      'optimizer_state_dict' : optimizer.state_dict(),\n","      'epoch' : i,\n","      'lr': current_lr\n","    }, '/kaggle/working/gta_bisenet_FDA_checkpoint.pth')\n","    print(f\"Current Learning Rate: {current_lr:.6f}\")\n"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:39:31.456854Z","iopub.status.busy":"2024-07-02T20:39:31.456472Z","iopub.status.idle":"2024-07-02T20:40:24.994083Z","shell.execute_reply":"2024-07-02T20:40:24.993068Z","shell.execute_reply.started":"2024-07-02T20:39:31.456825Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'Final mIoU:': 0.26429145032691437}\n","road : 77.6584%\n","sidewalk : 21.5107%\n","building : 73.2378%\n","wall : 19.9345%\n","fence : 6.2037%\n","pole : 21.4600%\n","light : 12.2506%\n","sign : 5.0337%\n","vegetation : 79.6623%\n","terrain : 20.0299%\n","sky : 80.5993%\n","person : 29.2181%\n","rider : 0.4521%\n","car : 41.4112%\n","truck : 7.2434%\n","bus : 2.8852%\n","train : 0.0000%\n","motocycle : 3.3183%\n","bicycle : 0.0445%\n"]}],"source":["# Validation\n","classes = ['road', 'sidewalk', 'building', 'wall', 'fence', 'pole', 'light', 'sign','vegetation', 'terrain', 'sky','person',\n","        'rider', 'car','truck','bus','train', 'motocycle','bicycle'] \n","\n","# Validation Set: Cityscapes\n","val_dataset = CityScapes(root_dir_val, split='val', transform_img=transform_img,transform_lab=transform_label)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=2)\n","\n","val = val(bisenet_model, val_loader, device)\n","for i in range(len(classes)) : \n","    print(f'{classes[i]} : {val[i]*100:.4f}%')"]},{"cell_type":"markdown","metadata":{},"source":["# DACS"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:49:42.216276Z","iopub.status.busy":"2024-07-02T20:49:42.215701Z","iopub.status.idle":"2024-07-02T20:49:45.241958Z","shell.execute_reply":"2024-07-02T20:49:45.241108Z","shell.execute_reply.started":"2024-07-02T20:49:42.216244Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|██████████| 44.7M/44.7M [00:00<00:00, 143MB/s] \n","Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n","100%|██████████| 171M/171M [00:01<00:00, 155MB/s]  \n"]}],"source":["bisenet_model = BiSeNet(num_classes=num_classes, context_path='resnet18')"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:49:46.177737Z","iopub.status.busy":"2024-07-02T20:49:46.177057Z","iopub.status.idle":"2024-07-02T20:49:46.426407Z","shell.execute_reply":"2024-07-02T20:49:46.425454Z","shell.execute_reply.started":"2024-07-02T20:49:46.177705Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using 2 GPUs!\n"]}],"source":["# Multiple GPU\n","device= \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","bisenet_model=bisenet_model.to(device)\n","if torch.cuda.device_count()>1:\n","    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n","    bisenet_model=nn.DataParallel(bisenet_model)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:49:48.807508Z","iopub.status.busy":"2024-07-02T20:49:48.807150Z","iopub.status.idle":"2024-07-02T20:49:48.813361Z","shell.execute_reply":"2024-07-02T20:49:48.812450Z","shell.execute_reply.started":"2024-07-02T20:49:48.807478Z"},"trusted":true},"outputs":[],"source":["# Loss and optimizer\n","loss_fn = nn.CrossEntropyLoss(ignore_index=255).to(device)\n","optimizer = torch.optim.Adam(bisenet_model.parameters(), lr=1e-3)\n","init_lr = 1e-3"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:49:53.263607Z","iopub.status.busy":"2024-07-02T20:49:53.263122Z","iopub.status.idle":"2024-07-02T20:49:53.285096Z","shell.execute_reply":"2024-07-02T20:49:53.284143Z","shell.execute_reply.started":"2024-07-02T20:49:53.263569Z"},"trusted":true},"outputs":[],"source":["#Function to generate masks\n","def generate_cutout_mask(img_size, seed=None):\n","    np.random.seed(seed)\n","    cutout_area = img_size[0] * img_size[1] / 2\n","    w = np.random.randint(img_size[1] / 2, img_size[1] + 1)\n","    h = np.round(cutout_area / w)\n","    x_start = np.random.randint(0, img_size[1] - w + 1)\n","    y_start = np.random.randint(0, img_size[0] - h + 1)\n","    x_end = int(x_start + w)\n","    y_end = int(y_start + h)\n","    mask = np.ones(img_size)\n","    mask[y_start:y_end, x_start:x_end] = 0\n","    return mask.astype(float)\n","\n","# mix Function\n","def oneMix(mask, data=None, target=None):\n","    if data is not None:\n","        stackedMask0, _ = torch.broadcast_tensors(mask, data[0])\n","        data = (stackedMask0 * data[0] + (1 - stackedMask0) * data[1])\n","    if target is not None:\n","        stackedMask0, _ = torch.broadcast_tensors(mask, target[0])\n","        target = (stackedMask0 * target[0] + (1 - stackedMask0) * target[1])\n","    return data, target\n","\n","# Transform definition GTA5\n","transform_img_gta5 = transforms.Compose([\n","    transforms.Resize((512, 1024)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean = [0.485, 0.456, 0.406],\n","                          std = [0.229, 0.224, 0.225])\n","])\n","\n","def transform_label_gta5(label):\n","    label = F.resize(label, (512, 1024), interpolation=Image.NEAREST)\n","    label = np.array(label, dtype=np.int64)\n","    return label\n","\n","# Transform definition Cityscapes\n","transform_img_cityscapes = transforms.Compose([\n","    transforms.Resize((512, 1024)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean = [0.485, 0.456, 0.406],\n","                          std = [0.229, 0.224, 0.225])\n","])\n","\n","def transform_label_cityscapes(label):\n","    label = F.resize(label, (512, 1024), interpolation=Image.NEAREST)\n","    label = np.array(label, dtype=np.int64)\n","    return label\n","\n","# Function that selects a class\n","def select_class_from_label(label, class_id):\n","    mask = label == class_id\n","    return mask\n","\n","# Function to mix selected classes\n","def mix_classes(city_image, city_label, gta_image, gta_label, class_id):\n","    gta_mask = select_class_from_label(gta_label, class_id)\n","    gta_mask = torch.tensor(gta_mask, dtype=torch.float32).clone().detach()\n","    gta_mask = gta_mask.unsqueeze(0)  # Aggiunge una dimensione per il broadcasting\n","    gta_image = gta_image * gta_mask\n","    gta_label = gta_label * gta_mask.squeeze(0)\n","    city_image_mixed = gta_mask * gta_image + (1 - gta_mask) * city_image\n","    city_label_mixed = gta_mask * gta_label + (1 - gta_mask) * city_label\n","    return city_image_mixed, city_label_mixed\n","\n","def transform_paired(image, label):\n","    random_aug = random.random()\n","    if random_aug < 0.5:\n","        image = F.hflip(image)\n","        label = F.hflip(label)\n","    return image, label"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:50:06.800708Z","iopub.status.busy":"2024-07-02T20:50:06.800353Z","iopub.status.idle":"2024-07-02T20:50:07.191271Z","shell.execute_reply":"2024-07-02T20:50:07.190347Z","shell.execute_reply.started":"2024-07-02T20:50:06.800681Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Training dataset size (GTA5): 2500\n","Training dataset size (Cityscapes): 1572\n"]}],"source":["# Dataset & Dataloaders\n","root_dir_gta5 = '/kaggle/input/gtadataset/GTA5'\n","root_dir_cityscapes = '/kaggle/input/citiscapes/Cityscapes/Cityspaces'\n","\n","batch_size = 4\n","\n","# Datasets\n","train_dataset_gta5 = GTA5(root_dir_gta5, split='train', transform_img=transform_img_gta5, transform_lab=transform_label_gta5)\n","train_dataset_cityscapes = CityScapes(root_dir_cityscapes, split='train', transform_img=transform_img_cityscapes, transform_lab=transform_label_cityscapes)\n","# Dataloaders\n","train_loader_gta5 = DataLoader(train_dataset_gta5, batch_size = batch_size, shuffle=True)\n","train_loader_cityscapes = DataLoader(train_dataset_cityscapes, batch_size = batch_size, shuffle=True)\n","\n","# Sizes\n","train_size_gta5 = len(train_dataset_gta5)\n","train_size_cityscapes = len(train_dataset_cityscapes)\n","print(\"Training dataset size (GTA5):\", train_size_gta5)\n","print(\"Training dataset size (Cityscapes):\", train_size_cityscapes)"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:48:37.817899Z","iopub.status.busy":"2024-07-02T20:48:37.816890Z","iopub.status.idle":"2024-07-02T20:48:37.831929Z","shell.execute_reply":"2024-07-02T20:48:37.830993Z","shell.execute_reply.started":"2024-07-02T20:48:37.817865Z"},"trusted":true},"outputs":[],"source":["# DACS Training Function\n","def train_DACS(model, optimizer, dataloader_GTA, val_loader, loss_fn, device, is_gta5=False):\n","    model.train()\n","    hist = np.zeros((19, 19))\n","    train_dataset_gta5 = GTA5(root_dir_gta5, split='train', transform_img=transform_img_gta5, transform_lab=transform_label_gta5)\n","\n","    val_iter = iter(val_loader)\n","\n","    for batch_idx, (gta_images, gta_labels) in enumerate(dataloader_GTA):\n","        #print('Batch ', batch_idx)\n","        try:\n","            city_images, city_labels = next(val_iter)\n","        except StopIteration:\n","            val_iter = iter(val_loader)\n","            city_images, city_labels = next(val_iter)\n","\n","        gta_images, gta_labels = gta_images.to(device), gta_labels.to(device).long()\n","        city_images, city_labels = city_images.to(device), city_labels.to(device).long()\n","\n","        batch, channels, height, width = city_images.shape\n","        city_images_tensor = []\n","        city_labels_tensor = []\n","\n","        for city_image, city_label, gta_image, gta_label in zip(city_images, city_labels, gta_images, gta_labels) :\n","            class_Ids = list(range(0, 20))\n","            class_id = random.choice(class_Ids)\n","            class_Ids.remove(class_id)\n","\n","            mixed_image, mixed_label = mix_classes(city_image, city_label, gta_image, gta_label, class_id)\n","            for class_id in random.sample(class_Ids, k=len(class_Ids)//2):\n","                mixed_image, mixed_label = transform_paired(mixed_image, mixed_label)\n","                mixed_label = mixed_label.long()\n","                mixed_image, mixed_label = mix_classes(mixed_image, mixed_label, gta_image, gta_label, class_id)\n","\n","            mixed_image, mixed_label = transform_paired(mixed_image, mixed_label)\n","            mixed_label = mixed_label.long()\n","            city_images_tensor.append(mixed_image)\n","            city_labels_tensor.append(mixed_label)\n","\n","        city_images_tensor = torch.stack(city_images_tensor)\n","        city_labels_tensor = torch.stack(city_labels_tensor)\n","        city_labels_tensor = city_labels_tensor.squeeze(1)\n","\n","        outputs = model(city_images_tensor)\n","        city_labels_tensor = city_labels_tensor.to(outputs[0].device)\n","        loss = loss_fn(outputs[0], city_labels_tensor)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        _, predicted = outputs[0].max(1)\n","        hist += fast_hist(city_labels_tensor.cpu().flatten().numpy(), predicted.cpu().flatten().numpy(), 19)\n","    print(per_class_iou(hist))\n","    miou = np.mean(per_class_iou(hist))\n","    print({\"Final mIoU:\": miou})"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:50:13.959038Z","iopub.status.busy":"2024-07-02T20:50:13.958706Z","iopub.status.idle":"2024-07-02T20:50:15.182041Z","shell.execute_reply":"2024-07-02T20:50:15.181028Z","shell.execute_reply.started":"2024-07-02T20:50:13.959013Z"},"trusted":true},"outputs":[],"source":["# Upload the already trained model\n","'''\n","checkpoint = torch.load('/kaggle/input/project/MLDL2024_project1-master/gta_bisenet_DACS_checkpoint_final.pth')\n","bisenet_model.load_state_dict(checkpoint['model_state_dict'])\n","optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","last_lr = checkpoint['lr']\n","start_epoch = checkpoint['epoch'] + 1\n","optimizer = torch.optim.Adam(bisenet_model.parameters(), lr=1e-3)\n","print(\"Start Epoch:\", start_epoch)\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["start_epoch = 0"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_dataset_gta5 = GTA5(root_dir_gta5, split='train', transform_img=transform_img_gta5, transform_lab=transform_label_gta5)\n","train_dataset_cityscapes = CityScapes(root_dir_cityscapes, split='train', transform_img=transform_img_cityscapes, transform_lab=transform_label_cityscapes)\n","\n","# Training\n","for i in range(start_epoch, 50) :\n","    #virtually expanded\n","    train_loader_gta5 = DataLoader(train_dataset_gta5, batch_size=8, shuffle=True)\n","    vtrain_loader_cityscapes = DataLoader(train_dataset_cityscapes, batch_size=8, shuffle=True)\n","    \n","    print(f'Epoch: {i}')\n","    current_lr = poly_lr_scheduler(optimizer, init_lr, iter = i)\n","    train_DACS(bisenet_model, optimizer, train_loader_gta5, train_loader_cityscapes, loss_fn, device, is_gta5 = True)\n","    torch.save({\n","          'model_state_dict' : bisenet_model.state_dict(),\n","          'optimizer_state_dict' : optimizer.state_dict(),\n","          'epoch' : i,\n","          'lr': current_lr\n","        }, '/kaggle/working/gta_bisenet_DACS_checkpoint.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'Final mIoU:': 0.3212515447123231}\n","road : 88.2905%\n","sidewalk : 30.8621%\n","building : 76.9870%\n","wall : 21.5801%\n","fence : 13.7093%\n","pole : 25.7958%\n","light : 13.7701%\n","sign : 14.9595%\n","vegetation : 70.6447%\n","terrain : 13.2385%\n","sky : 64.4308%\n","person : 48.1232%\n","rider : 0.6920%\n","car : 80.5979%\n","truck : 14.5439%\n","bus : 21.4569%\n","train : 7.0231%\n","motocycle : 3.6725%\n","bicycle : 0.0000%\n"]}],"source":["# Validation\n","classes = ['road', 'sidewalk', 'building', 'wall', 'fence', 'pole', 'light', 'sign','vegetation', 'terrain', 'sky','person',\n","        'rider', 'car','truck','bus','train', 'motocycle','bicycle'] \n","\n","root_dir_val = '/kaggle/input/citiscapes/Cityscapes/Cityspaces'\n","val_dataset = CityScapes(root_dir_val, split='val', transform_img=transform_img_cityscapes,transform_lab=transform_label_cityscapes)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=2)\n","\n","val = val(bisenet_model, val_loader, device)\n","for i in range(len(classes)) : \n","    print(f'{classes[i]} : {val[i]*100:.4f}%')"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyM8+VRarmkqnEpw1uSRUjw1","gpuType":"T4","mount_file_id":"1b_n_z97yp0E4SBnoUKayFheXcm6jgW0f","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4950640,"sourceId":8335926,"sourceType":"datasetVersion"},{"datasetId":4950804,"sourceId":8336145,"sourceType":"datasetVersion"},{"datasetId":5042189,"sourceId":8459144,"sourceType":"datasetVersion"},{"datasetId":4966616,"sourceId":8712429,"sourceType":"datasetVersion"}],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
